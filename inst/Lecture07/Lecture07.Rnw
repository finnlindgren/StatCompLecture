\documentclass[compress,handout,t,10pt,fleqn,aspectratio=169]{beamer}

\input{common.tex}
\input{commonbeamer.tex}
\togglefalse{byhand}
\toggletrue{byhand}
<<echo=FALSE>>=
solutions <- TRUE
opts_chunk$set(fig.path = 'figure/L07-',
               fig.align = 'center',
               fig.show = 'hold',
               size = 'footnotesize',
               fig.width = 8,
               fig.height = 5.2,
               out.width = "\\linewidth",
               out.height = "0.65\\linewidth")
knit_hooks$set(inline = function(x) {
  if (is.numeric(x)) return(knitr:::format_sci(x, 'latex'))
  highr::hi_latex(x)
})
set.seed(12345L)
@
% Handle solutions inclusion toggle.
\togglefalse{solutions}
<<echo=FALSE, results="asis">>=
if (solutions) {
  cat("\\toggletrue{solutions}")
}
@

\newcommand{\mc}[1]{\mathcal{#1}}

\begin{document}


\begin{frame}[fragile]
  \frametitle{Cross validation, Bootstrap, and Tests \hfill\small(MATH10093)\\\large Model assessment}
  ~\\
  Cross validation:
  \begin{itemize}
  \item Data splitting revisited
  \item Uncertainty for the expected test score
  \item Multiple splitting and cross validation
  \end{itemize}
  Bootstrap
  \begin{itemize}
  \item Data resampling
  \item Bias and variance estimation for estimators
  \item Parametric bootstrap
  \end{itemize}
  Exploiting exchangability
  \begin{itemize}
  \item Residual resampling, basic exchangeability
  \item Randomisation/permutation tests
  \end{itemize}
  ~
  \vfill
  ~
\end{frame}




<<echo=FALSE>>=
# Input:
#   pred : data.frame with (at least) a column "mu"
#   y : data vector
score_se <- function(pred, y) {
  (y - pred$mu)^2
}
# Input:
#   pred : data.frame with (at least) columns "mu" and "sigma"
#   y : data vector
score_ds <- function(pred, y) {
  ((y - pred$mu) / pred$sigma)^2 + 2 * log(pred$sigma)
}
@


\begin{frame}
\frametitle{Proper scores and data splits}
\begin{itemize}
\item
Recall the expectation of a score for a forecast $F$ under a true distribution $G$,
\begin{align*}
S(F, G) &= \pE_{y\sim G}[S(F, y)] .
\end{align*}\vspace*{-4mm}
\item A (negatively oriented) score is \emph{proper} if $S(F, G) \geq S(G, G)$ for all predictions $F$.
\item In simulation studies, we know the true $G$, so if we can simulate from it or do direct calculations, we can estimate or compute $S(F,G)$ for a given forecast $F$.
\item In real applications, $G$ is unknown.
\end{itemize}
Before, we split the data in subsets for \emph{observation}/\emph{estimation}/\emph{training},
$\mc{Y}^\text{train}$, and \emph{test}, $\mc{Y}^\text{test}$.
\begin{block}{Basic estimation and testing with data splitting}
\begin{itemize}
\item Estimate the model using $\mc{Y}^\text{train}$
\item Construct forecasts $F^\text{test}_i$ for $y_i$ in $\mc{Y}^\text{test}$.
\item Estimate $\ol{S}(\{F^\text{test}_i\},\{G^\text{test}_i\})$ with\\ $\ol{S}(\{F^\text{test}_i\},\{y^\text{test}_i\})=\frac{1}{|\mc{Y}^\text{test}|}\sum_{i=1}^{|\mc{Y}^\text{test}|} S(F^\text{test}_i,y^\text{test}_i)$
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\begin{block}{Generalised splitting}
\begin{itemize}
\item Observation/Estimation/Training\hfill
Data used to estimate a model
\item Validation\hfill
Data for assessing estimates and taking modelling decisions
\item Test\hfill
Data used in a final step to assess the resulting model
\end{itemize}
\end{block}
~\\
\begin{itemize}
\item
Decisions based on scores evaluated on Training or Validation data might lead to overestimation of the predictive ability.
\item
\emph{Holding out} a separate Test set provides a safer way of assessing predictive ability.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Basic score uncertainty}
\begin{itemize}
\item We're interested in the expected average prediction test score $\ol{S}(F^\text{test},G^\text{test})$
\item Before looking at the Test data, we only have access to an \emph{estimate} of $\ol{S}(F^\text{test},G^\text{test})$
, based on a training/validation data split:
\begin{align*}
\wh{S}^\text{valid} &= \frac{1}{N}\sum_{i=1}^N S(F^\text{valid}_i,y^\text{valid}_i)
\end{align*}
\item Note: To investigate the \emph{difference} in expected score between two models or methods, just replace $S(F_i,y_i)$ by the pairwise differences $S^\Delta_i=S^\Delta(F_i,F'_i,y_i)=S(F_i,y_i)-S(F'_i,y_i)$ everywhere.
\item The empirical variance estimate for $\wh{S}^\text{valid}$ is
\begin{align*}
\wh{\pVar}[\wh{S}^\text{valid}] &= \frac{1}{N(N-1)}\sum_{i=1}^N \left[
S(F^\text{valid}_i,y^\text{valid}_i)
-\wh{S}^\text{valid}
\right]^2
\end{align*}
\item The variance estimate may be biased due to dependence between the scores.
\end{itemize}
\end{frame}


\newcommand{\DD}{{\mathcal{D}}}
\begin{frame}
\frametitle{Cross validation}
\begin{itemize}
\item We're interested in the expected average prediction test score $\ol{S}(F^\text{test},G^\text{test})$ when using all the Training and Validation data to estimate the parameters of the final model.
\item The Training set is a subset; may lead to overestimation of the expected score
\item The Validation set is a small subset; high variability in the score estimator
\item Different splits might give different score estimates and hence different modelling decisions
\item Partial solution: Do multiple splits
\end{itemize}
~\vspace*{10mm}
\begin{block}{K-fold Cross Validation: CV(K)}
\begin{itemize}
\item Split the $N$ data points $\DD$ into $K$ subsets $\DD_k^\text{(K)}$, each of size $N/K$.
\item Iterate over the $K$ subsets, treating each as a Validation set, $\DD_k^\text{valid}=\DD_k^\text{(K)}$,\\ and the remaining $K-1$ subsets as Training data $\DD_k^\text{train}=\cup_{j\neq k} \DD_j^\text{(K)}$.
\item Average over the resulting $K$ score estimates.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Cross-validation scores}
\begin{itemize}
\item
For each of the $K$ \emph{folds}, the estimator of the expected score is
\begin{align*}
\wh{S}^\text{(K)}_k&=\frac{K}{N}\sum_{i=1}^{N/K} S(F^\text{valid}_{ki},y^\text{valid}_{ki})
\end{align*}
\item The combined cross-validation score is
\begin{align*}
\wh{S}^\text{CV(K)}&=\frac{1}{K}\sum_{k=1}^{K} \wh{S}^\text{(K)}_k
\end{align*}
\item There are many options for estimating the variance of the combined CV score. Simple:
\begin{align*}
\wh{\pVar}[\wh{S}^\text{CV(K)}] &=
\frac{1}{K(K-1)}\sum_{k=1}^{K} [\wh{S}_k^\text{(K)} - \wh{S}^\text{CV(K)}]^2
\end{align*}
\item No universal rule for what $K$ and splitting choices will minimise\\
the bias and variance of the estimators.\\
Common choice is $K=10$ and random splitting.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Common problem-dependent splitting options}
\begin{itemize}
\item Leave-one-out CV; LOOCV$=$CV(N)\\
In general very expensive, but for some model classes fast approximations are possible;
Notably in Gaussian time series and spatial models
\item Structured, only partially random, cross-validation examples:
\begin{itemize}
\item Leave-station-out (to assess spatial predictive ability)
\item Leave-country-out (to assess macro scale generalisability,
including potentially different measurement systems)
\item Leave-timepoint-out (to assess temporal interpolation ability)
\item Leave-individual-out (to assess generalisability of medical treatment between patients)
\item Related non-cross-validation example: Leave-future-out (to assess forecasting ability)
\end{itemize}
\item Alternative: Instead of complete splitting, do multiple Validation subset selections as random subsamples with replacement (related to \emph{Bootstrap})
\end{itemize}
\end{frame}


\begin{comment}
\begin{frame}
\frametitle{R live coding example and debugging}
For lab 6, we need code to:
\begin{itemize}
\item Create random cross validation split
\item Estimate each CV model
\item Compute the validation score for each model
\item Combine the results, make model selection
\item (Compute the test score for the final model)
\end{itemize}
\vfill
Some debugging tools:
\begin{itemize}
\item \code{traceback()}: Where did my code fail? What function calls did it use?
\item \code{debugonce(fun)}: I'm feeling lucky and might find the error in \code{fun()} straight away!
\item \code{debug(fun)}: The function \code{fun} with the problem is called in a loop and I need to run until I see the problem.
\item \code{undebug(fun)}: Please stop debugging!
\item \code{browser()}: Stop here and continue interactively inside the function!
\end{itemize}
\end{frame}


\begin{frame}[fragile]
<<eval=FALSE>>=
# create_split:
# Construct an group index vector for cross validation splitting into K groups
create_split <- function(data, K) {
  indices <- rep(1:10, times = nrow(data) / 10, size = nrow(data))
  sample(indices, size = nrow(mydata), replace = FALSE)
}

# Read the lab 6 data:
TMINallobs <- read.csv(file = "data/TMINallobs.csv",
                       header = TRUE,
                       stringsAsFactors = FALSE)
# To simplify the example we shorten the data to a nice round number:
mydata <- TMINallobs[1:10000, ]

# Try to construct a data split
thesplit <- create_split(mydata, 5)
unique(thesplit)
# [1] 3 10  2  5  4  6  1  8  9  7

# If this was someone else's function, we might use debugonce()
# to step into it to find out what's wrong (we expected values between 1 and 5):
debugonce(create_split)
create_split(mydata, 5)
# Press Enter to run each line in turn. See ?browser for more commands.
@
\end{frame}
\end{comment}












<<echo=FALSE>>=
# Input:
#   pred : data.frame with (at least) a column "mu"
#   y : data vector
score_se <- function(pred, y) {
  (y - pred$mu)^2
}
# Input:
#   pred : data.frame with (at least) columns "mu" and "sigma"
#   y : data vector
score_ds <- function(pred, y) {
  ((y - pred$mu) / pred$sigma)^2 + 2 * log(pred$sigma)
}
@


\begin{frame}
\frametitle{Cross validation and Bootstrap}
\begin{itemize}
\item Cross validation splits the data in $K$ parts and performs model estimation on $(K-1)$ parts and validation assessment on the $K$th part, all for each of the parts.
\item Bootstrap resamples \emph{with replacement} to obtain a random sample of the same size as the original sample.
\end{itemize}
\begin{block}{Basic Bootstrap resampling}
Let $Y = \{(y_i, x_i), i=1,\dots,N\}$ be a data collection with response values $y_i$ and predictors/covariates $x_i$.
\begin{itemize}
\item Define a \emph{Bootstrap sample} $Y^{(j)}$ by drawing $N$ pairs $(y_i,x_i)$ from $Y$ with equal probability, and with replacement.\\
\item Repeat this procedure for $j=1,\dots,J$, with $J\gg 1$.
\end{itemize}
\end{block}
The resampling procedure draws a random sample from\\
the \emph{empirical distribution} for the data collection.
\end{frame}

\begin{frame}
\frametitle{The Bootstrap principle}
\begin{itemize}
\item Each boostrap sample $Y^{(j)}$ can be used to apply some model estimation procedure, each generating a parameter estimate $\wh{\theta}^{(j)}$.
\item We want to use these bootstrapped estimates to say something about the properties of the estimator $\wh{\theta}$ which is based on the original data $Y$.
\item Idea: The parameter estimate is a deterministic function of the data, the \emph{empirical parameter value} for the observed sample $Y$:
$\wh{\theta}=\theta(Y)$ and
$\wh{\theta}^{(j)}=\theta(Y^{(j)})$.
\end{itemize}
\begin{block}{The Bootstrap principle}
According to the \emph{Bootstrap principle}, the errors of the bootstrapped estimates have the same distribution as the error of $\wh{\theta}$.
In particular, if the true parameter is $\theta_\text{true}$, then
\begin{align*}
\pE(\wh{\theta} - \theta_\text{true}) & = \pE(\wh{\theta}^{(j)} - \wh{\theta}), \\
\pVar(\wh{\theta} - \theta_\text{true}) & = \pVar(\wh{\theta}^{(j)} - \wh{\theta}).
\end{align*}\vspace*{-3mm}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Bootstrap estimation}
\begin{itemize}
\item The usual expectation and variance estimators can be used:
\begin{align*}
\wh{\pE}(\wh{\theta} - \theta_\text{true}) & = \frac{1}{J}\sum_{j=1}^J(\wh{\theta}^{(j)} - \wh{\theta}) = \ol{\wh{\theta}^{(\cdot)}} - \wh{\theta}, &
\wh{\pVar}(\wh{\theta} - \theta_\text{true}) & = \frac{1}{J-1}\sum_{j=1}^J\left(\wh{\theta}^{(j)} - \ol{\wh{\theta}^{(\cdot)}}\right)^2.
\end{align*}
\item Bias adjusted estimator $\wh{\theta}-\left(\ol{\wh{\theta}^{(\cdot)}} - \wh{\theta}\right)$. Properties? Need \emph{double bootstrap}!
\item Confidence intervals for $\theta_\text{true}$: Consider the quantiles of the error distribution:\\
Find $a$ and $b$ such that $\pP(a < \wh{\theta}^{(j)}-\widehat{\theta} < b) = 1-\alpha$ from the empirical quantiles of the Bootstrap sample residuals
$\wh{\theta}^{(j)}-\widehat{\theta}$.\\
According to the Bootstrap principle,
$$
\pP(a < \wh{\theta}^{(j)}-\widehat{\theta} < b) = \pP(a < \wh{\theta}-\theta_\text{true} < b),
$$
so that
$$
\pP(\wh{\theta}-b < \theta_\text{true} < \wh{\theta} - a) = 1-\alpha,
$$
and a confidence interval is given by
$$
\text{CI}_\text{boot}(\theta) = \left(\wh{\theta}-b,\wh{\theta}-a\right)
$$
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Alternative confidence interval derivation}
Instead of using the empirical quantiles of the boostrap \emph{residuals}, we can use the samples themselves:\\
Find $A$ and $B$ such that $\pP(A < \wh{\theta}^{(j)} < B) = 1-\alpha$ from the empirical quantiles of the Bootstrap samples $\wh{\theta}^{(j)}$.\\
According to the Bootstrap principle,
$$
\pP(A < \wh{\theta}^{(j)} < B)
= \pP(A -\wh{\theta} < \wh{\theta}^{(j)}-\wh{\theta} < B - \wh{\theta})
= \pP(A - \wh{\theta} < \wh{\theta}-\theta_\text{true} < B - \wh{\theta}),
$$
so that
$$
\pP(2\wh{\theta}-B < \theta_\text{true} < 2\wh{\theta} - A) = 1-\alpha,
$$
and a confidence interval is given by
$$
\text{CI}_\text{boot}(\theta) = \left(2\wh{\theta}-B,2\wh{\theta}-A\right) .
$$
\end{frame}

\begin{frame}
\frametitle{Bootstrap is not posterior sampling}
\begin{itemize}
\item
In Bayesian statistics, \emph{Credible Intervals} can be obtained as lower and upper
empirical quantiles of samples from the \emph{posterior distribution}.
\item
It could therefore be tempting to just use the empirical Bootstrap sample quantiles $(A,B)$ as the interval, but this would not work in the presence of Bias or skewness of the Bootstrap distributions.
\item Instead, the \emph{Bootstrap confidence interval} construction involves the
\emph{upper} quantile of the Bootstrap sample in the \emph{lower} endpoint,
and vice versa.
\end{itemize}
\end{frame}














<<echo=FALSE>>=
# Input:
#   pred : data.frame with (at least) a column "mu"
#   y : data vector
score_se <- function(pred, y) {
  (y - pred$mu)^2
}
# Input:
#   pred : data.frame with (at least) columns "mu" and "sigma"
#   y : data vector
score_ds <- function(pred, y) {
  ((y - pred$mu) / pred$sigma)^2 + 2 * log(pred$sigma)
}
@


\begin{frame}
\frametitle{Parametric bootstrap}
\begin{itemize}
\item If the data size is small, basic Boostrap that resamples with replacement from the raw data has very little information.
\item An alternative is to sample entirely new data from the model that was estimated on the whole data set.
\end{itemize}
\begin{block}{Parametric Bootstrap sampling}
Let $Y = \{(y_i, x_i), i=1,\dots,N\}$ be a data collection with response values $y_i$ and predictors/covariates $x_i$, such that the conditional data density function is $p(y_i|x_i,\theta)$.
Let $\wh{\theta}$ be the maximum likelihood estimate of $\theta$.
\begin{itemize}
\item Define the \emph{Bootstrap sample} $Y^{(j)}$ by drawing $N$ pairs $(y_i^{(j)},x_i)$, where for each $x_i$, $y_i^{(j)}$ is drawn from $p(y_i|x_i,\wh{\theta})$.
\item Repeat this procedure for $j=1,\dots,J$, with $J\gg 1$.
\end{itemize}
Depending on the problem, the $x_i$ values can either be kept fixed to their\\
values in the original data set, resampled with replacement, or simulated\\
from some generative model.
\end{block}
\end{frame}

\begin{frame}
\frametitle{Exchangeability}
\begin{itemize}
\item We can relax model assumptions, e.g.\ if we don't trust a Gaussian assumption for regression residuals.
\item Two model components can be said to be \emph{exchangeable} is swapping their values
gives the same joint distribution.
\item Instead of assuming a specific residual distribution (usually Gaussian), we can relax the assumption to saying only that the residuals all have the same, but unknown, distribution; the individual residuals are (probabilistically) indistinguishable.
\item Instead of resampling the raw data, we resample the model \emph{residuals}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Residual resampling}
\begin{block}{Residual resampling in regression models}
Let $Y = \{(y_i, x_i), i=1,\dots,N\}$ be a data collection with response values $y_i$ and predictors/covariates $x_i$, such that $\mu_i=\pE(y_i|x_i,\theta)$ is the conditional expectation in a regression model.
Let $\wh{\theta}$ be the maximum likelihood estimate of $\theta$, and $\wh{\mu}_i=\pE(y_i|x_i,\wh{\theta})$.
\begin{itemize}
\item Define the residuals $r_i = y_i-\wh{\mu}_i$, and construct a \emph{residual Bootstrap sample} $Y^{(j)}$ by drawing $N$ pairs $(y_i^{(j)},x_i)$, where for each $x_i$, $y_i^{(j)}=\wh{\mu}_i + r_i^{(j)}$, where $r_i^{(j)}$ is drawn with replacement from the empirical distributon of $\{r_1,r_2,\dots,r_N\}$.
\item Repeat this procedure for $j=1,\dots,J$, with $J\gg 1$.
\end{itemize}
For $x_i$, the same options as for fully parametric Bootstrap are available.
\end{block}
\end{frame}

\begin{frame}
\frametitle{Randomisation/permutation tests}
When assessing differences between two statistical populations, the hypotheses often take the form of some kind of \emph{exchangeability structure}.
\begin{block}{Exchangeability test example}
\begin{itemize}
\item
Let $Y_A=\{y^A_1,\dots,y^A_{N_A}\}$ and
$Y_B=\{y^B_1,\dots,y^B_{N_B}\}$, and we want to test the hypotheses
\begin{align*}
H_0&: \text{The $A$ and $B$ come from the same distribution}\\
H_1&: \text{The $A$ and $B$ do not come from the same distribution}
\end{align*}\vspace*{-4mm}
\item Given a test statistic $T(Y_A,Y_B)$, such as $\ol{y^A_\cdot}-\ol{y^B_\cdot}$, we need the distribution of $T$ under $H_0$.
\item
Under $H_0$, the joint sample $Y_{A\cup B}=\{y^A_1,\dots,y^A_{N_A},y^B_1,\dots,y^B_{N_B}\}$ is a collection of exchangeable variables.
\item Each random permutation of $Y_{A\cup B}$ has the same distribution\\
as $Y_{A\cup B}$, under $H_0$ (but not under $H_1$).
\end{itemize}
\end{block}
\end{frame}


\begin{frame}
\frametitle{Randomisation/permutation tests}
Assume that large test statistics $T$ indicate deviations from $H_0$.
\begin{block}{Permutation tests}
\begin{itemize}
\item For $j=1,\dots,J$, draw $Y_{A\cup B}^{(j)}$ as a random permutation of $Y_{A\cup B}$, and split the result into subsets $Y_A^{(j)}$ and $Y_B^{(j)}$ of size $N_A$ and $N_B$, respectively.
\item Compute the test statistics $T^{(j)}=T(Y_A^{(j)},Y_B^{(j)})$.
\item The average $\frac{1}{J}\sum_{j=1}^J \II\{T^{(j)} \geq T(Y_A,Y_B)\}$ is an unbiased estimator of the p-value w.r.t.\ $T$ for the hypothesis $H_0:\text{the elements of $Y_A$ and $Y_B$ are mutually exchangeable}$.
\item If $N_A+N_B$ is small, the limit $J\rightarrow\infty$ can be obtained by using all possible permutations instead of independent random permutations.
\end{itemize}
\end{block}
\begin{itemize}
\item In other exchangeability situations, similar tests can be designed.
\end{itemize}
\end{frame}




\begin{frame}
\frametitle{Randomisation/permutation test for proper scores}
\begin{itemize}
\item
In a collection of prediction scores for two models $A$ and $B$,
each data point has its own predictive distribution, so we don't have full exchangeability.
\item We do have \emph{pairwise exchangeability} if the two model predictions are equivalent.
\item To construct a formal test, we randomise the scores within each pair. If we investigate the difference between scores, $S^\Delta_i=S(F^A_i,y_i)-S(F^B_i,y_i)$, $i=1,\dots,N$, that means swapping the sign of the difference. For testing the average difference, we can use
test statistic
$$
T(\{S^{\Delta}_i\}) = \frac{1}{N}\sum_{i=1}^N S^\Delta_i
$$
\item For each $j=1,\dots,J$ and $i=1,\dots,N$, draw $S^{\Delta(j)}_i=S^\Delta_i$ with probability $0.5$, and $-S^\Delta_i$ with probability $0.5$.
\item Compute the test statistics, $T^{(j)}=T(\{S^{\Delta(j)}_i,i=1,\dots,N\})$.
\item The average $\frac{1}{J}\sum_{j=1}^J \II\{T^{(j)} \geq T(\{S^{\Delta}_i\})\}$ is an unbiased estimator of\\ the one-sided p-value w.r.t.\ $T$ for the hypothesis\\
$H_0:\text{the scores of the two models are pairwise exchangeable}$ vs.\\
$H_1:\text{Model $B$ is better than $A$}$
\end{itemize}
\end{frame}


\begin{comment}
\begin{frame}
\frametitle{Statistical Computing summary}
\begin{itemize}
\item R data structures, indexing
\item Structured programming
\item Numerical maximum likelihood estimation
\item Proper scoring functions for assessing predictions
\item Floating point computation error analysis
\item Cross validation
\item Bootstrap
\item Permutation tests
\item Integration, basic Bayesian computation
\end{itemize}
\end{frame}
\end{comment}

\end{document}
